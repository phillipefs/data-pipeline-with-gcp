name: 'Deploy Python File to GCP Bucket'

on:
  push:
    branches:
    - master
    paths:
      - 'etl-spark/main.py'
  pull_request:
    branches:
    - master
    paths:
      - 'etl-spark/main.py'

jobs:
  deploy_script:
    name: 'Deploy Script'
    runs-on: ubuntu-latest

    steps:
      - name: Checkout
        uses: actions/checkout@v2

      - name: Setup Python
        uses: actions/setup-python@v2
        with:
          python-version: '3.x' # ou a versão que você precisa

      - name: Install Google Cloud SDK
        run: |
          echo "deb [signed-by=/usr/share/keyrings/cloud.google.gpg] https://packages.cloud.google.com/apt cloud-sdk main" | sudo tee -a /etc/apt/sources.list.d/google-cloud-sdk.list
          sudo apt-get install apt-transport-https ca-certificates gnupg
          curl https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key --keyring /usr/share/keyrings/cloud.google.gpg add -
          sudo apt-get update && sudo apt-get install google-cloud-sdk

      - name: Set up Google Cloud SDK
        run: gcloud auth configure-docker

      - name: Copy file to GCP bucket
        env:
          GOOGLE_APPLICATION_CREDENTIALS: ${{ github.workspace }}/temp_credentials.json
        run: |
          echo "${{ secrets.GOOGLE_CREDENTIALS }}" > temp_credentials.json
          gsutil cp etl-spark/main.py gs://data-pipeline-combustiveis-br-pyspark-code/